{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0739a8-eaf0-40ee-99d3-bd5e2d3257f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q swig PyOpenGL PyOpenGL_accelerate\n",
    "#!pip install --upgrade moviepy imageio-ffmpeg pyvirtualdisplay piglet\n",
    "#!pip install \"gymnasium[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8babd2b-9142-4abc-89fc-e57c678b7fba",
   "metadata": {},
   "source": [
    "### Gymnasium interface\n",
    "\n",
    "The three main methods of an environment are\n",
    "* `reset()`: reset environment to the initial state, return first observation and dict with auxiliary info\n",
    "* `render()`: show current environment state (a more colorful version :) )\n",
    "* `step(a)`: commit action `a` and return `(new_observation, reward, terminated, truncated, info)`\n",
    " * `new_observation`: an observation right after committing the action `a`\n",
    " * `reward`: a number representing your reward for committing action `a`\n",
    " * `terminated`: True if the MDP has just finished, False if still in progress\n",
    " * `truncated`: True if the number of steps elapsed >= max episode steps\n",
    " * `info`: some auxiliary stuff about what just happened. For now, ignore it.\n",
    "\n",
    "A detailed explanation of the difference between `terminated` and `truncated` and how it should be used:\n",
    "1. https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/\n",
    "2. https://gymnasium.farama.org/content/migration-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d711bb27-d4dd-4e75-a124-fb8d6fcca392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 환경 클래스 정의\n",
    "class Iowa_Gambling_Task(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(Iowa_Gambling_Task, self).__init__() ## 부모 클래스 생성자 호출\n",
    "        \n",
    "        ## variable\n",
    "        self.action_space = gym.spaces.Discrete(4) ## A,B,C,D 4가지 선택지\n",
    "        self.score = 2000 ##사람이 가지고 있는 돈 ## 처음 시작하는 돈 \n",
    "        self.observation_space = 0 ## 임의로 0으로 둠\n",
    "        self.current_step = 0 ## 현재 step ## 첫 step = 0\n",
    "        \n",
    "        ## memeory\n",
    "        self.choices = [] ## 지금까지 한 선택들\n",
    "        self.rewards = [] ## 지금까지 받은 reward들\n",
    "\n",
    "    def create_deck_rewards(self, action):\n",
    "        ## reward\n",
    "        positive_rewards ={ \n",
    "            0 : 100, #A\n",
    "            1 : 100, #B\n",
    "            2 : 50, #C\n",
    "            3 : 50  #D   \n",
    "        }\n",
    "        \n",
    "        negative_rewards={ ## lambda : 쓰는 이유: 선택할 때마다 바뀌게 하기 위해서\n",
    "            0 : lambda : np.random.choice([0, 150, 200, 250, 300, 350], p =[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]),## A \n",
    "            1 : lambda : np.random.choice([0, 1250], p =[0.9, 0.1]),## B\n",
    "            2 : lambda : np.random.choice([0, 25, 75, 50], p =[0.5, 0.1, 0.1, 0.3]), ## C\n",
    "            3 : lambda : np.random.choice([0, 250], p =[0.9, 0.1]) ## D\n",
    "        }\n",
    "\n",
    "        return positive_rewards[action], negative_rewards[action]()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.rewards = []\n",
    "        self.choices = []\n",
    "        self.current_step = 0\n",
    "        self.score = 2000 \n",
    "        return 0, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        ## reward 계산\n",
    "        pos_r , neg_r = self.create_deck_rewards(action)\n",
    "        reward = pos_r -  neg_r\n",
    "        \n",
    "        ## memory\n",
    "        self.choices.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        \n",
    "        ## reward 반영\n",
    "        self.score += reward\n",
    "        self.current_step += 1 ## 현재 step 갱신\n",
    "\n",
    "        ## 게임 끝났는 지 판단\n",
    "        done = False \n",
    "        if self.score<=0:\n",
    "            done = True\n",
    "\n",
    "        ## info\n",
    "        info = {\n",
    "            \"positive_reward\" : pos_r,\n",
    "            \"negative_reward\" : neg_r\n",
    "        }\n",
    "        \n",
    "        return 0, reward, done, info\n",
    "\n",
    "    def get_history(self): ## 선택과 reward return\n",
    "        return self.choices, self.rewards\n",
    "\n",
    "    def get_score(self): ## score return\n",
    "        return self.score\n",
    "    \n",
    "    def render(self): ## render\n",
    "        print(f\"Step : {self.current_step}, Score : {self.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85144217-6267-48b3-b98e-de17637714ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 점검\n",
    "env = Iowa_Gambling_Task()\n",
    "obs = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "print(env.choices)\n",
    "print(env.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b13fd2-ca04-4dd3-961d-f6749d4c668a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
